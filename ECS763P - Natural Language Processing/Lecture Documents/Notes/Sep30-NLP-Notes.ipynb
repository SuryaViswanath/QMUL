{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de36cc68-ace2-4555-a9d3-ee5df2530d1b",
   "metadata": {},
   "source": [
    "# [ECS763P] - Natural Language Processing\n",
    "\n",
    "## Lecture 2 notes:\n",
    "\n",
    "Running points:\n",
    "-  Preprocessing text helps in removing unnecessary information\n",
    "-  shakespeare corpus used `'d` for past tennse >>intersting<<\n",
    "-  Maximum matching algorithm works well with chinese language, but not really with the english. So understand and analyse the language to see which one to use\n",
    "-  **language is ambiguous**; There are a lot of problems in the pre-processing. Look at those\n",
    "-  Why is stemming the way it is? who defines the rules for these things? why is it faster? Compare and evaluate the lemmatization and stemming\n",
    "-  so does stemming and lemmatization not work for anything other than classification?\n",
    "-  how can we optimise the model generalisation over data\n",
    "-  What are features? how to choose the features and what are the different ways to identify new features and also pootentially create new ones\n",
    "-  Evaluation metrics, how to understand\n",
    "-  how to choose weights\n",
    "-  Error Analysis - how to balance the dataset so that the classification works well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2b0f3-808d-4a7b-8562-112acfeec2c2",
   "metadata": {},
   "source": [
    "## Run these cells before starting your reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e116db-97af-4451-bda9-91086952171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yourTake(userPhrase):\n",
    "    words_set = {'grouping', 'organizing', 'labeling', 'dividing', 'arranging', \n",
    "             'assigning', 'sorting', 'categorizing', 'classifying', 'separating'}\n",
    "    if any(word in userPhrase.split() for word in words_set):\n",
    "        return \"Nice work, let's expand on that\"\n",
    "    else:\n",
    "        return \"Close, let's dive deeper into it\"\n",
    "\n",
    "\n",
    "def smartyPants(userPhrase):\n",
    "    if ('ec' in userPhrase) or ('first 2' in userPhrase):\n",
    "        display(Image(filename='example.gif'))\n",
    "        return \"Exactly, the first 2 characters\"\n",
    "    else:\n",
    "        return \"In our case the first 2 characters help us\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f6453d-9a91-47f0-a3cd-f53553460f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/darksurrealya/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/darksurrealya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def highlight_words(s):\n",
    "    styles = []\n",
    "    for index, row in s.iterrows():\n",
    "        if row['Word'] != row['Stemmed']:\n",
    "            styles.append(['color: blue', 'color: green', 'color: red'])\n",
    "        else:\n",
    "            styles.append(['color: green', 'color: green', 'color: green'])\n",
    "    return pd.DataFrame(styles, index=s.index, columns=s.columns)\n",
    "\n",
    "def checkLemmaAndStemma(sentence):\n",
    "    tokens = sentence.lower().strip().split()\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    lemmatized = []\n",
    "    \n",
    "    for word, pos in pos_tags:\n",
    "        wordnet_pos = get_wordnet_pos(pos)\n",
    "        if wordnet_pos:\n",
    "            lemmatized.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "        else:\n",
    "            lemmatized.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Word': tokens,\n",
    "        'Stemmed': stemmed,\n",
    "        'Lemmatized': lemmatized\n",
    "    })\n",
    "    \n",
    "    return df.style.apply(highlight_words, axis=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cb68393-d63b-41ef-a3dc-f35f34339551",
   "metadata": {},
   "source": [
    "## Let's get started\n",
    "\n",
    "When you are working with problems associated with the language, we rely on the techniques associated with the natural language processing. This ability to understand the language helps in solving complex problems. \n",
    "\n",
    "   <div style=\"text-align: center;\">\n",
    "       <img src=\"images/power-meme.jpg\" alt=\"Example Image\" width=\"300\" height=\"150\"/>\n",
    "   </div>\n",
    "\n",
    "\n",
    "\n",
    "As the saying goes, `With great power comes great responsibility`, Lets give it a new form in the case of NLP -> `With great solution comes, great problems.`\n",
    "\n",
    "Solving a language problem comes with it's own challenges for ex: \n",
    "1. **Inconsistent use of words** - varying usage of words\n",
    "   <div style=\"text-align: center;\">\n",
    "       <img src=\"images/incosistent-use-of-words.png\" alt=\"Example Image\" width=\"300\" height=\"150\"/>\n",
    "   </div>\n",
    "\n",
    "2. **Linguistic variations** - meaning is similar (**_it is similar, not the same_**), the way in which it is constructed is different\n",
    "   <div style=\"text-align: center;\">\n",
    "       <img src=\"images/similar-sentences.png\" alt=\"Example Image\" width=\"500\" height=\"300\"/>\n",
    "   </div>\n",
    "\n",
    "3. **Redundant information in data** - most often than not, there is a lot of information that is repeated so much so that the corpus/corpora should be cleaned. One such example is **stopword removal**\n",
    "   <div style=\"text-align: center;\">\n",
    "       <img src=\"images/stopwords.png\" alt=\"Example Image\" width=\"120\" height=\"150\"/>\n",
    "   </div>\n",
    "\n",
    "4. **Tokenization** - Tokenization is an important step during data preprocessing as it helps the model understand the data much better. However, what you need to understand here is there are a lot of nuances. These nuances will be covered just in a little while.\n",
    "\n",
    "5. **Normalization** - In simple terms, it is making sense of what is given. There are two different types at a high level:\n",
    "   - Symmetric normalization\n",
    "   - Asymmetric normalization\n",
    "\n",
    "   More in detail a little later.\n",
    "\n",
    "6. **Sentence segmentation** - When punctuation comes in your way, and not having it is the best. We will dive a little deeper into this too in the following sections.\n",
    "\n",
    "\n",
    "Now that we kind of outlined that there are a lof of challenges, let's put all these problems under 1 umbrella. So let me think of a name to give\n",
    "\n",
    "...thinkinnnnnnng\n",
    "\n",
    "...thinkinnnnnnnggggggg\n",
    "\n",
    "...thinn\n",
    "\n",
    "> **Text Preprocessing**\n",
    "\n",
    "Voila!!! Chef's kiss\n",
    "\n",
    "Alright coming back, I know I will bomb my first stand up gig but hey! I am doing my education so hopefully I earn better (not)\n",
    "\n",
    "So now, coming back to the story. This `Text Preprocessing` plays a crucial role in getting the data to perform for instance a text classification problem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Even though there are a lot of problems, all these problems have solutions if we understand fully well what is causing the issue in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d3213c-faf9-4f8d-9b85-f3f3498c174b",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenization is the dividing the sentences in such a way that the model is able to understand the data. This is very important because it helps with learning patterns from the corpus and build model that could predict the class label accurately. \n",
    "\n",
    "But don’t let this fool you—it’s not just a quick slice with a regex knife! You’ve got to keep those pieces in shape so they don’t lose meaning. I mean, what’s the point of slicing up \"England, Englands, and England’s\" if your model thinks they’re three different countries? \n",
    "\n",
    "   <div style=\"text-align: center;\">\n",
    "       <img src=\"images/tokenization-issues.png\" alt=\"Example Image\" width=\"350\" height=\"200\"/>\n",
    "   </div>\n",
    "\n",
    "\n",
    "If you see the image carefully, how do you put a finger and say the rule for tokenizing \n",
    "\n",
    "Not easy right!\n",
    "\n",
    "There are two different ways of creating your own tokenizer\n",
    "1. Top down tokenizer - it is like the dictator, you set rules before looking at the data from your own understanding of what it had to say and do the tokenization. \n",
    "    One of the best example of this is the **Penn Treebank Tokenizer** from the Linguistic Data Consortium\n",
    "\n",
    "2. Bottom up tokenizer - This is the most common\n",
    "\n",
    "Instead of just chopping up text by words or letters, modern language models play it smart by using \"subwords\" – breaking words into smaller, meaningful bits. So, if they’ve never seen \"lower\" before, they’ll simply use \"low\" and \"er\" like puzzle pieces. **Byte-pair encoding (BPE)** and **unigram models** are the tools they use, teaching the model to combine and split chunks like pros. It's like making your model a linguistic master of Tetris!\n",
    "\n",
    "\n",
    "\n",
    "### Some Common Tokenizers\n",
    "\n",
    "| Tokenizer        | Library          | Features                                        |\n",
    "|------------------|------------------|-------------------------------------------------|\n",
    "| <span style=\"color:green\">NLTK Tokenizer</span>   | NLTK             | Word, sentence, regex, treebank tokenization    |\n",
    "| <span style=\"color:blue\">SpaCy Tokenizer</span>  | SpaCy            | Efficient, customizable, multi-language support |\n",
    "| <span style=\"color:#DAA520\">Transformers</span>     | Hugging Face     | Subword tokenization (BPE, WordPiece, Unigram)  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7214f91-e4b2-49ab-8a06-e49b17a92834",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Normalization in our case is trying to solve the problem that we see in the tokenization, how can you get a semblance of what the word is all about. This idea of getting to understand the word by trimming things down to it's true identity is what normalization is. Keep in mind this doesn't fully solve the problem of tokenization, but gives us lot less to worry about. \n",
    "\n",
    "So if we try to dive deep into this, we come to some techniques called lemmatization and stemming\n",
    "\n",
    "\n",
    "**Lemmatization:**\n",
    "Lemmatization is the language detective—it tracks words back to their roots like \"ran\" and \"running,\" declaring, “Aha! It’s 'run' all along!” Always solving cases with perfect grammar.\n",
    "\n",
    "**Stemming:**\n",
    "Stemming is the word hacker—it’ll cut \"playing\" down to \"play\" or \"amazing\" to \"amaz\" without a second thought. Precise? Not really. Efficient? You bet!\n",
    "\n",
    "Try the following exercise to understand more about the lemmatization and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d101e0a8-58ff-4190-9439-27490d8fbd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Monsieur/Madame what word do you want to use I am shivering and I know it\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_4fa6c_row0_col0, #T_4fa6c_row0_col1, #T_4fa6c_row0_col2, #T_4fa6c_row1_col0, #T_4fa6c_row1_col1, #T_4fa6c_row1_col2, #T_4fa6c_row2_col1, #T_4fa6c_row3_col0, #T_4fa6c_row3_col1, #T_4fa6c_row3_col2, #T_4fa6c_row4_col0, #T_4fa6c_row4_col1, #T_4fa6c_row4_col2, #T_4fa6c_row5_col0, #T_4fa6c_row5_col1, #T_4fa6c_row5_col2, #T_4fa6c_row6_col0, #T_4fa6c_row6_col1, #T_4fa6c_row6_col2 {\n",
       "  color: green;\n",
       "}\n",
       "#T_4fa6c_row2_col0 {\n",
       "  color: blue;\n",
       "}\n",
       "#T_4fa6c_row2_col2 {\n",
       "  color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_4fa6c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4fa6c_level0_col0\" class=\"col_heading level0 col0\" >Word</th>\n",
       "      <th id=\"T_4fa6c_level0_col1\" class=\"col_heading level0 col1\" >Stemmed</th>\n",
       "      <th id=\"T_4fa6c_level0_col2\" class=\"col_heading level0 col2\" >Lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4fa6c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_4fa6c_row0_col0\" class=\"data row0 col0\" >i</td>\n",
       "      <td id=\"T_4fa6c_row0_col1\" class=\"data row0 col1\" >i</td>\n",
       "      <td id=\"T_4fa6c_row0_col2\" class=\"data row0 col2\" >i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4fa6c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_4fa6c_row1_col0\" class=\"data row1 col0\" >am</td>\n",
       "      <td id=\"T_4fa6c_row1_col1\" class=\"data row1 col1\" >am</td>\n",
       "      <td id=\"T_4fa6c_row1_col2\" class=\"data row1 col2\" >be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4fa6c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_4fa6c_row2_col0\" class=\"data row2 col0\" >shivering</td>\n",
       "      <td id=\"T_4fa6c_row2_col1\" class=\"data row2 col1\" >shiver</td>\n",
       "      <td id=\"T_4fa6c_row2_col2\" class=\"data row2 col2\" >shiver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4fa6c_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_4fa6c_row3_col0\" class=\"data row3 col0\" >and</td>\n",
       "      <td id=\"T_4fa6c_row3_col1\" class=\"data row3 col1\" >and</td>\n",
       "      <td id=\"T_4fa6c_row3_col2\" class=\"data row3 col2\" >and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4fa6c_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_4fa6c_row4_col0\" class=\"data row4 col0\" >i</td>\n",
       "      <td id=\"T_4fa6c_row4_col1\" class=\"data row4 col1\" >i</td>\n",
       "      <td id=\"T_4fa6c_row4_col2\" class=\"data row4 col2\" >i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4fa6c_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_4fa6c_row5_col0\" class=\"data row5 col0\" >know</td>\n",
       "      <td id=\"T_4fa6c_row5_col1\" class=\"data row5 col1\" >know</td>\n",
       "      <td id=\"T_4fa6c_row5_col2\" class=\"data row5 col2\" >know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4fa6c_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_4fa6c_row6_col0\" class=\"data row6 col0\" >it</td>\n",
       "      <td id=\"T_4fa6c_row6_col1\" class=\"data row6 col1\" >it</td>\n",
       "      <td id=\"T_4fa6c_row6_col2\" class=\"data row6 col2\" >it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1057377f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkLemmaAndStemma(input(\"Monsieur/Madame what word do you want to use\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b42100-8cf1-4f64-b9ac-ee04aa6c6276",
   "metadata": {},
   "source": [
    "**Before we dive into the what is text classification problem. Write down what you understand of text classification in a sentence?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb99d233-45c4-497e-95f9-73a8a9529745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What do you understand from text classification? grouping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Nice work, let's expand on that\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yourTake(input('What do you understand from text classification?').lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75357b6a-6ff5-4631-be0f-852109bc85e5",
   "metadata": {},
   "source": [
    "### So what is Text classification?\n",
    "![Text Classification](images/what-is-text-classification.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31bccca-1284-4134-b89f-eeb4f212784f",
   "metadata": {},
   "source": [
    "Text classification is classifying the text ot it's concerned group. For instance in the case of students, based on the id number provided there is information that would reflect which department they belong to - ecxxxxxx is your number so I know you belong to the stream of electronics and computer science department.\n",
    "\n",
    "but wait did you not get the question that I got while writing this example - `How do you know that the id belonged to the Electronics and Computer science department`. If you know please answer below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12a5c342-0f40-4686-8683-6257cd21ef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Can you try guessing it? ec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Exactly, the first 2 characters'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smartyPants(input(\"Can you try guessing it?\").lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ba93c-8a00-4842-8bde-fde58a5f2e0f",
   "metadata": {},
   "source": [
    "Hence there are 2 ways in which we can train the model\n",
    "1. Supervised Learning\n",
    "2. Unsupervised Learning\n",
    "\n",
    "\n",
    "The id card example is a case of supervised learning. Today let us dive a little into how supervised learning works\n",
    "\n",
    "\n",
    "_Text preprocessing_ that was done prior helps the machine to identify the patterns and there by assign a designated label to which it may belong. This way of a controlled environment where you know what it belongs to is called as a supervised learning\n",
    "\n",
    "Hence, \n",
    "<p style=\"text-align: center;\">\n",
    "    <img src = \"images/Text_Classification_Machine_Learning_NLP.png\", alt = \"Classification\" width = \"500\"/>\n",
    "</p>\n",
    "Here the dataset is represented as d, and the final labels as C, hence for any new example that comes our way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3754f8d-a887-4023-8fad-f1695b301b1e",
   "metadata": {},
   "source": [
    "$$\n",
    "c^* \\in C\n",
    "$$\n",
    "where:\n",
    "- $( C )$ is the set of all possible classes.\n",
    "- $( c^* )$ is the target class that the algorithm predicts for a given input \\( x \\).\n",
    "\n",
    "The classification task can be summarized as finding a function ( f: x &rarr; C)\n",
    " that maps input data points $( x )$ from the feature space $( X )$ to their corresponding classes in $( C )$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a7afa9-3236-4643-9726-fe9d93e10e83",
   "metadata": {},
   "source": [
    "So yeah, that's about it; that is how we train the model. Goodbye, thank you!<br>\n",
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"images/bye-girl.gif\" alt=\"Bye\" width=\"300\"/> <!-- Adjust the width as needed -->\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43446e72-ce85-43fa-811f-d50b6a9dca87",
   "metadata": {},
   "source": [
    "Well not so easy my friend, this doesn't end our problems. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let us now dive into what it takees to do the classification, what are the stages that are there before we get the final product out\n",
    "\n",
    "![Stages of development](images/stages.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e42eea-9053-4fa6-9d1a-53804097b1c1",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    ">_disclaimer: feature is nothing but a column in the dataset_\n",
    "\n",
    "Feature engineering is when you try different costumes to see what works best for you, if need be you create something new like getting a haircut or buying a shoe just for complimenting the look. Feature engineering just like that needs your help to see what information from the dataset helps your model to better perform.\n",
    "\n",
    "but how would you know whether that feature/ column information is helping your model. This can be done using some **Empircal Evaluation**, they are\n",
    "- Incremental testing: adding one feature at a time to see the performance of the model\n",
    "- Leave-one-out testing: out of all the columns, if you think a column wouldn't really help then remove and see (trial and error)\n",
    "- Error analysis: it is about retrospecting on the mistake after making one, you look at the performance and see how the data is impacting it. For instance, a good example would be having an imbalanced dataset that you have not handled before developing the model. We fix that and come back here.\n",
    "\n",
    "You can learn more about feature engineering here: https://www.kaggle.com/learn/feature-engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c5a2e3-0891-468e-b2a0-70d8429ee7c8",
   "metadata": {},
   "source": [
    "#### Performance evaluation\n",
    "There are various ways of evaluating a model, firstly we need to understand what kind of problem we have at our hand and then consider the evaluation metric.\n",
    "\n",
    "For instance, if we consider a **Binary Classification**\n",
    "\n",
    "1. Precision:\n",
    "\n",
    "How it helps: Measures the accuracy of positive predictions. High precision means fewer false positives, which is crucial in scenarios where false alarms are costly (e.g., spam detection).\n",
    "\n",
    "2. Recall:\n",
    "\n",
    "How it helps: Measures the ability to capture all actual positives. High recall is vital when missing a positive instance is detrimental (e.g., in medical diagnoses, where missing a disease can be harmful).\n",
    "\n",
    "3. F1 Score:\n",
    "\n",
    "How it helps: Combines precision and recall into a single score to assess model performance, especially useful in imbalanced datasets. A high F1 score indicates a good balance between precision and recall, ensuring that both false positives and false negatives are minimized.\n",
    "\n",
    "So in short:\n",
    "- Precision focuses on minimizing false positives.\n",
    "- Recall emphasizes minimizing false negatives.\n",
    "- F1 Score balances both metrics, making it ideal for evaluating models where both types of errors matter.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/matrix.png\" alt=\"Image 2\" width=\"700\"/></td>\n",
    "        <td><img src=\"images/f1score.png\" alt=\"Image 1\" width=\"700\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "$$generally \\ \\beta = 1$$\n",
    "\n",
    "\n",
    "\n",
    "In the case of **Multiclass Classification**, we use something called as the **confusion matrix**\n",
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"images/confusion-matrix.png\" alt=\"Image 2\" width=\"700\"/>\n",
    "</p>\n",
    "\n",
    "This is really helpful to understand how a particular column is performing, and how many of them are getting wrongly labelled. This helps us in making a decision whether we want to correct the model such that it doesn't go into other labels or not.\n",
    "\n",
    "> Here’s where an important concept comes into play: **overfitting**. Overfitting occurs when you try to train a model on more data to help it learn specific patterns better. However, this can be problematic because the model may misclassify many labels simply because it recognizes a pattern that is somewhat similar to the actual label.\n",
    "\n",
    "Coming back to the evaluation of multiclass, we also calculate precision and recall to see how well it is able to learn and retain information on the unseen data. These are calculated using\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"images/mc-pre-re.png\" alt=\"Image 2\" width=\"700\"/>\n",
    "</p>\n",
    "\n",
    "When evaluating the performance of classification models, especially in multi-class settings, it’s important to summarize individual class metrics into a single score. Two common techniques for doing this are **micro averaging** and **macro averaging**. Here’s a brief overview of both methods:\n",
    "\n",
    "**Micro Averaging**\n",
    "Micro averaging aggregates the contributions of all classes to compute the average metric. It does this by summing the true positives, false positives, and false negatives across all classes before calculating the precision, recall, or F1 score. This approach treats each instance equally, making it particularly useful when the class distribution is imbalanced.\n",
    "\n",
    "- Advantages:\n",
    "\n",
    "    - Provides a more balanced view when dealing with imbalanced datasets.\n",
    "    - Highlights the model's performance on the majority class.\n",
    "\n",
    "      Example: If there are three classes with the following confusion matrix:\n",
    "        \n",
    "          Class A: 10 TP, 5 FP, 2 FN\n",
    "          Class B: 15 TP, 2 FP, 3 FN\n",
    "          Class C: 20 TP, 3 FP, 1 FN\n",
    "        Micro averages would sum these values across all classes and then compute metrics.\n",
    "\n",
    "**Macro Averaging**\n",
    "Macro averaging, on the other hand, calculates the metric independently for each class and then takes the average of those values. This means that all classes contribute equally to the final score, regardless of their support (the number of instances in each class). This method is beneficial when you want to evaluate the model's performance across all classes equally, especially if they have different importance or relevance.\n",
    "\n",
    "- Advantages:\n",
    "\n",
    "    - Gives equal weight to all classes, which can be crucial when some classes are more important than others.\n",
    "    - Useful for understanding the model’s performance on minority classes.\n",
    "\n",
    "      Example: Using the same confusion matrix:\n",
    "        \n",
    "          Class A: Precision = TP / (TP + FP)\n",
    "          Class B: Precision = TP / (TP + FP)\n",
    "          Class C: Precision = TP / (TP + FP)\n",
    "        The macro average precision would be the mean of the precision scores from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc225e7-aed0-45c2-ab20-910ae29de341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c54e64-87af-4886-a9aa-1c50cdc4238f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17634a0a-6afa-4da0-8f23-49eb0663473a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260d687d-af0f-4ee8-96bc-a2a40b50a347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a997fba-d9f1-4749-9b7d-0df8913baf84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec3f1c-e289-4a7a-83b8-f95bd3082b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29882a8a-de78-4b80-aeb2-22fb895c3a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
